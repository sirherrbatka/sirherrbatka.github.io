#+TITLE:       Introducing cl-data-frames
#+AUTHOR:
#+EMAIL:       shka@tuxls
#+DATE:        2019-09-25 śro
#+URI:         /blog/%y/%m/%d/introducing-cl-data-frames
#+KEYWORDS:    lisp
#+TAGS:        lisp
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:nil \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: Data frame library for the Common Lisp

* A little bit of background
The key feature of commonly used languages for data science and statistical analysis is the existence of data frames. Data frame is essentially just a way of presenting data in the form of a table, plus a set of operations allowing easy manipulation of the content. This probably does not sound all that thrilling at this point but once the domain is considered one quickly realizes that in practice almost every frequently performed task should be performed using a data frame. Don't get me wrong: It still can be done without one, but by sticking to just the fundamental data structures you are simply wasting your time. Let's, for instance, consider a typical code written in R with the help of the ubiquitous Dplyr package.

We have a data frame containing data set with a human body type information: weight and height under the columns "weight" and "height". We decided to add a new column to the data frame with a body mass index (BMI for short, and that's how the new column will be called). To do so, we will simply need a single line of the code (assuming that dataset uses metric units).

#+BEGIN_SRC R
transform(body_data_frame, bmi = weight / height^2)
#+END_SRC

You may be wondering: how does this even work? There is no lambda form to be found and the result of calling the transform is not stored anywhere. Well, unless you know R already! Dplyr makes good use of R features: first-class expressions and the first-class environments. First-class expressions are somewhat like first-class functions. Just like some languages allow you to pass a function as an argument to another function, R allows you to pass non-evaluated expression. The difference is that while functions are embedded in the lexical environment which is augmented during the call by creating new bindings according to the lambda list of the function, expressions are not called; they are just evaluated. R allows you to pass an explicit environment where this will happen as well as facilities to programmatically construct environment during the runtime. Transform iterates row-wise over the whole frame, each time evaluating the passed expression in an environment containing additional bindings to the values in this row, under each column. After evaluation values can be accessed, manipulated and transferred to a new data frame. That is, new. Transform does not perform destructive manipulation of the existing data frame. Instead, it will mutate the outer environment to alter the binding of the body_data_frame so it will point to a newly constructed data frame (in addition to this dplyr provides completly non-destructive function (sic!) mutate that does not mutate anything). This is a quite elegant (and maybe a little bit magical) solution!

Unfortunately, R is not ideal. Even after the initial learning curve while acquainting with the essential packages you may find yourself limited. For instance: you may ask yourself "where are the input/output streams?" and the answer is "there are none". R is an highly specialized language and lacks many of the general-purpose features that you may take for granted. This may seem to be a not a practical issue initially, but eventually, you will find yourself attempting to represent sets of boolean flags as a bit-set. You will then realize that you won't be able to force any of the libraries you are using for such important tasks like clustering to work with your new data representation.

Python's pandas library is not nearly as nice to use. Spark's data frames even less so. Also, neither of those is lisp. Perhaps Julia is the right answer, but I am already in a deep romantic relationship with the Common Lisp. And not only that: there is a lot of practical merit in using Common Lisp in all sorts of interactive data processing tasks. Hover, as already established, I would prefer to do so using a data frames library, and so I decided to write one that would be pleasant to use.

This blog post describes this new library of the excelent name "cl-data-structures".
* Design goals
What "pleasant to use" means though? Such vague statement is not a proper goal statement. So what are the exact things I am looking for?
1. I work with a sparse datasets. Data frames should be sparse to not waste memory when represeting mostly null columns.
2. Design should be layered in a such a way that it becomes possible to quickly hack in needed features. For instance, It should be possible to construct data frames in a way simple enough to integrate with other libraries. This is important because I can't even hope to support every possible data source. But If user can, for isntance; fill the data frame while iterating over postmodern query result with a DOQUERY in just few extra lines of code, this becomes much less of a problem.
3. Efficient and *safe* data sharing between different instances of data frames. This should allow memory usage to be compact even while working with a large data sets.
4. API should be simple. Ideally most of the use cases should be realised with just a few functions. For instance: it is desirable that all row wise operations are performed with a single function, akin to the Dplyr mutate/transform.
5. Both destructive (but *safe*, I don't want to end up with a clobbered data just because condition popped up) and non-destructive operations.
* Decissions, decissions…
With having in mind those goals, I could move forward and instantly the new questions have risen. What layers should be distinguished in the library and what are the concepts expressed by each layer? How data representation is structured? What basic operations are supplied? Which data structures should be used?
* General architecture
Cl-data-structures is composed of layers connected by sets of protocol functions. Those layers are, from the lowest level to the highest level:
todo
* Key concepts
* Data structures
Sparsity requirement combined with the need for both destructive and non-destructive operations convinced me to use column representation based around the sparse variation of the RRB vector data structure. Sparsity was achieved by adding a bitmask into every RRB tree node to mark occupied node (just like in the HAMT data structure). This increases memory requirements for each non-leaf node in the trie, but only by a 32 bits extra for each node. In fact I consider this be good enough to not even bother with a dense variant of the column. Luckly I had those already in the cl-data-structures, but theres was still plenty of code extra required for an efficient implementation of data frames.
* High level API
* Use cases
* The future
