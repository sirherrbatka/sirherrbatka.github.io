#+TITLE:       Introducing cl-data-frames
#+AUTHOR:
#+EMAIL:       shka@tuxls
#+DATE:        2019-09-25 śro
#+URI:         /blog/%y/%m/%d/introducing-cl-data-frames
#+KEYWORDS:    lisp
#+TAGS:        lisp
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:nil \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: Data frame library for the Common Lisp

* A little bit of background
The key feature of commonly used languages for data science and statistical analysis is the existence of data frames. Data frame is essentially just a way of presenting data in the form of a table, plus a set of operations allowing easy manipulation of the content. This probably does not sound all that thrilling at this point but once the domain is considered one quickly realizes that in practice almost every frequently performed task should be performed using a data frame. Don't get me wrong: It still can be done without one, but by sticking to just the fundamental data structures you are simply wasting your time. Let's, for instance, consider a typical code written in R with the help of the ubiquitous Dplyr package.

We have a data frame containing data set with a human body type information: weight and height under the columns "weight" and "height". We decided to add a new column to the data frame with a body mass index (BMI for short, and that's how the new column will be called). To do so, we will simply need a single line of the code (assuming that dataset uses metric units).

#+BEGIN_SRC R
transform(body_data_frame, bmi = weight / height^2)
#+END_SRC

You may be wondering: how does this even work? There is no lambda form to be found and the result of calling the transform is not stored anywhere. Well, unless you know R already! Dplyr makes good use of R features: first-class expressions and the first-class environments. First-class expressions are somewhat like first-class functions. Just like some languages allow you to pass a function as an argument to another function, R allows you to pass non-evaluated expression. The difference is that while functions are embedded in the lexical environment which is augmented during the call by creating new bindings according to the lambda list of the function, expressions are not called; they are just evaluated. R allows you to pass an explicit environment where this will happen as well as facilities to programmatically construct environment during the runtime. Transform iterates row-wise over the whole frame, each time evaluating the passed expression in an environment containing additional bindings to the values in this row, under each column. After evaluation values can be accessed, manipulated and transferred to a new data frame. That is, new. Transform does not perform destructive manipulation of the existing data frame. Instead, it will mutate the outer environment to alter the binding of the body_data_frame so it will point to a newly constructed data frame (in addition to this dplyr provides completly non-destructive function (sic!) mutate that does not mutate anything). This is a quite elegant (and maybe a little bit magical) solution!

Unfortunately, R is not ideal. Even after the initial learning curve while acquainting with the essential packages you may find yourself limited. For instance: you may ask yourself "where are the input/output streams?" and the answer is "there are none". R is an highly specialized language and lacks many of the general-purpose features that you may take for granted. This may seem to be a not a practical issue initially, but eventually, you will find yourself attempting to represent sets of boolean flags as a bit-set. You will then realize that you won't be able to force any of the libraries you are using for such important tasks like clustering to work with your new data representation.

Python's pandas library is not nearly as nice to use. Spark's data frames even less so. Also, neither of those is lisp. Perhaps Julia is the right answer, but I am already in a deep romantic relationship with the Common Lisp. And not only that: there is a lot of practical merit in using Common Lisp in all sorts of interactive data processing tasks. Hover, as already established, I would prefer to do so using a data frames library, and so I decided to write one that would be pleasant to use.

This blog post describes this new library of the excelent name "cl-data-structures".
* Design goals
What "pleasant to use" means though? Such vague statement is not a proper goal statement. So what are the exact things I am looking for?
1. I work with a sparse datasets. Data frames should be sparse to not waste memory when represeting mostly null columns.
2. Design should be layered in a such a way that it becomes possible to quickly hack in needed features. For instance, It should be possible to construct data frames in a way simple enough to integrate with other libraries. This is important because I can't even hope to support every possible data source. But If user can, for isntance; fill the data frame while iterating over postmodern query result with a DOQUERY in just few extra lines of code, this becomes much less of a problem.
3. Efficient and *safe* data sharing between different instances of data frames. This should allow memory usage to be compact even while working with a large data sets.
4. API should be simple. Ideally most of the use cases should be realised with just a few functions. For instance: it is desirable that all row wise operations are performed with a single function, akin to the Dplyr mutate/transform.
5. Both destructive (but *safe*, I don't want to end up with a clobbered data just because condition popped up) and non-destructive operations.
* Decissions, decissions…
With having in mind those goals, I could move forward and instantly the new questions have risen. What layers should be distinguished in the library and what are the concepts expressed by each layer? How data representation is structured? What basic operations are supplied? Which data structures should be used?
* General architecture
Cl-data-structures is composed of layers connected by sets of protocol functions, objects, classes and variables. Those layers are, from the lowest level to the highest level:
1. Column layer.
2. Header layer.
3. Table layer.
4. API layer.
* Key concepts
Column layer provides basic data structure used to represent column as well as means to change content of it, either directly or by the use of the iterator. Iterator also provides means to alter multiple columns at once which is needed for building and modyfing whole data frames. Columns are mutable and they are using copy on write mechanism to save time and memory when multiple column instances share parts or the whole of the content.

Immutable headers represent the information on columns forming the data frame. This includes: types stored in the columns; column aliases and predicates for the content, or in other words: data schema. Many functions assume that relevant header is implicitly passed as a dynamic (also 'called' special) variable. This also means that quite frequently calls to cl-data-frame functions are placed within a with-header (or with-table) form that establishes such binding. This may appear to be inconvinient, hovewer this allows to maintain separation between header and actual content, and therefore operating on a file streams to a CSV file, therefore allowing for aggregation without need for the actual data frame to be constructed. This is extreamly useful when input file is larger then RAM available on the machine.

The same layer also establishes concept of the *current-row* which is an object bound to the dynamic variable during row wise operations, granting read and write access to the content of the currently processed data frame row. This features allow easier integration with the cl-data-structures library by separting content of the current row from returned and accepted arguments in the lambda forms passed to the cl-data-structures layre and agregation functions. This is demonstrated in the *Use cases* section of this post.

Table itself is a mutable object composed of header and sequence of columns. AT generic function allows to access value in the individual cell of the table, but it is strongly advised to not loop over the table, and instead use TRANSFORM function for row wise changes and a relevant function from the cl-data-structures package for an aggregation (for instance calculating sum of all elements in the column, finding minimum or maximum, and so one).
* Data structures
Sparsity requirement combined with the need for both destructive and non-destructive operations convinced me to use column representation based around the sparse variation of the RRB vector data structure. Sparsity was achieved by adding a bitmask into every RRB tree node to mark occupied node (just like in the HAMT data structure). This increases memory requirements for each non-leaf node in the trie, but only by a 32 bits extra for each node. In fact I consider this be good enough to not even bother with a dense variant of the column. Luckly I had those already in the cl-data-structures, but theres was still plenty of code extra required for an efficient implementation of operations needed by data frames.
* High level API
For the most typical tasks user is expected to simply stick to the high level API and do not dwell in the low level details described above. Therefore I've added separate package gathering relevant symbols from the layers below and rexporting along with added functionality. This API is composed of the following symbols:

1. EMPTY-TABLE
2. SAMPLE
3. COPY-FROM
4. COPY-TO
5. WITH-TABLE
6. WITH-HEADER
7. STANDARD-HEADER
8. MAKE-HEADER
9. AT
10. COLUMN-COUNT
11. ROW-COUNT
12. BODY
13. RR
14. BRR
15. MAKE-HEADER
16. TO-TABLE
17. VMASK
18. VSELECT
19. VSTACK
20. REPLICA
21. REMOVE-NULLS
22. PRINT-TABLE

I am not completly happy with shape of this API right now, but at least I am not bothered by backward compatibility. It is nice to have a fresh start but it is not easy to get everything right. Regardless: right now It is not even a horrible system to use, as the below examples should demonstrate.
* Use cases
This may seem to all nice, but I bet you wonder how this allows to solve practical problems you may encounter. Let me take you on a journey trough my own work and how I use this software to make my life easier.
* The future
